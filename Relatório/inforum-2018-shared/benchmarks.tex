


\section{Benchmarks e metodologia de testes} \label{benchs}

O controlo de redes ao nível do chamado \emph{Control Plane} baseia-se, por razões relacionadas com a necessidade de um elevado desempenho e de simplicidade, em protocolos de coordenação entre as diferentes componentes de rede (e.g. \emph{switches, routers, controladores \dots}) com níveis de consistência do tipo ``consistência eventual''. Por essa razão, o estado propagado ou computado pelas diferentes componentes da rede transita entre estados consistentes através de períodos de inconsistência \cite{  } mais ou menos prolongados.






Para tal foi criado um \textit{benchmark} baseado na análise do tempo de execução do ciclo principal da aplicação. Fazendo variar o débito e o número de elementos do sistema 
distribuído é possível analisar a capacidade de ambas as tecnologias de emular de uma rede virtual e também observar quais os limites desta.

Foi desenvolvida uma aplicação em python cujo objetivo consiste na inserção periódica de tuplos numa base de dados mysql local e periodicamente enviar os mesmos agroupadamente para serem inseridos em bases de dados remotas, replicando assim a base de dados. 

Este formato de testes foi construído de raíz, pois o que foi verificado no trabalho relacionado (ver \ref{relac}), é que por norma os testes de \textit{performance} entre máquinas virtuais e containers são a um nível local e não implicam qualquer tipo de comunicação. Visando este projecto a comparação de \textit{performances} de diferentes ambientes a um nível distribuído, foi então desenhada uma aplicação que representasse um sistema desse mesmo tipo.

O sistema onde foram efetuados os \textit{benchmarks} em questão, tinha dois processadores \textit{Intel Xeon E5-2670 v3} a 2.30GHz com 12 núcleos físicos (24 lógicos) cada, 128GB de RAM DDR4 2400MHz e corria o Hipervisor VMWare ESXi 6.5 diretamente sobre o hardware.

Para os testes do ambiente de containers Docker, foi criada uma máquina virtual que permitisse os testes correrem num aspeto semelhante a um sistema cloud, onde os sistemas de containers correm sobre uma máquina virtual por questões de segurança. Para máquina em questão, dos recursos previamente mencionados foram disponibilizados os 12 núcleos físicos de ambos os processadores e 96GB de RAM. Não foi selecionada toda a RAM disponível pois escolheu-se deixar alguma disponível para o hipervisor. O sistema operativo de eleição para a máquina virtual dos testes foi o CentOS 7.

\subsection{Funcionamento da aplicação} \label{func}

\begin{itemize}

\item Seja D o débito da aplicação, a razão entre a quantidade de tuplos inseridos e uma unidade temporal. D = $\frac{msg}{minuto}$
\item Seja AG o fator de agregação das mensagens enviadas para as bases de dados remotas
\item Seja N o número de instâncias que compõe o sistema distribuído

\end{itemize}


O fluxo da aplicação consiste em três estados principais:

\begin{enumerate}
\item Sincronização
\begin{description}
\item A fase de sincronização possui como objetivo a coordenação das múltiplas instâncias da aplicação. Esta consiste no estabelecimento de ligações entre os diversos componentes do sistema. Após estabelecidas as ligações, é inserida uma entrada numa tabela de sincronização pertencente á base de dados elemento cujo endereço IP é o menor (\textit{master node}). Posteriormente são efetuadas listagens dessa tabela periodicamente, no momento em que o número de entradas for igual ao número de elementos do sistema distribuído é concluído que todos os elementos do sistema estão prontos para prosseguir para a seguinte fase de execução.
\end{description}

\item Ciclo principal
\begin{description}
\item No ciclo principal periodicamente (de acordo com D), será gerado um número aleatório de 0-100 e gerada uma sequência aleatória de carateres. Um tuplo composto pelos elementos referidos anteriormente será inserido na tabela da base de dados local. Após AG inserções locais serem executadas, são inserido os mesmos tuplos agregados nas restantes N - 1 bases de dados. Cada instância da aplicação irá executar estas inserções, resultando na tabela referida anteriormente replicada em N bases de dados.
\end{description}

\item Dessincronização
\begin{description}
\item Na fase de dessincronização todos os elementos do sistema distribuído irão remover o tuplo inserido durante a fase de sincronização (presente na tabela do \textit{master node}), em seguida irá efetuar a listagem dessa tabela até que esta esteja vazia. Nesse momento é concluída a fase de dessincronização.
\end{description}

\end{enumerate}





